~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ASSIGNMENT
~~~~~~~~~~~Q1.Use a dataset of your choice from Open Data Portal (https:// data.gov.in/, UCI repository) .
Load a Pandas dataframe with a selected dataset. Identify and count the missing values in a dataframe.
Clean the data after removing noise as follows
a) Drop duplicate rows.
b) Detect the outliers and remove the rows having outliers
c) Identify the most correlated positively correlated attributes and negatively correlated attributes

import pandas as pd 
file_path = r"C:\Users\hp\Desktop\COLLEGE\SEM 3\DAV\RTYB1819_A3_4e (dataset).csv"
df = pd.read_csv(file_path)
column_names = df.columns
print("Column Names:")
print(column_names)

# Identify and count missing values
missing_values = df.isnull().sum()
print("Missing values in each column:")
print(missing_values)
# Drop duplicate rows
df = df.drop_duplicates()
# Select only numerical columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
# Loop through numerical columns to detect and print outliers
for column in numerical_columns:
    # Calculate the IQR (Interquartile Range)
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
# Create a mask to identify rows with outliers
    outlier_mask = ((df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR)))
# Check if there are any outliers in the column
    if outlier_mask.any():
        print(f"Column '{column}' has outliers.")

df_no_outliers = df[~outlier_mask]

# Display the cleaned DataFrame without outliers
#print("DataFrame without outliers:")
#print(df_no_outliers)
correlation_matrix = df.corr()
# For positive correlation
positive_corr = correlation_matrix[correlation_matrix > 0.5]
# For negative correlation
negative_corr = correlation_matrix[correlation_matrix < -0.5]
# Display results
print("Positively correlated attributes:")
print(positive_corr)
print("\nNegatively correlated attributes:")
print(negative_corr)


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Q2.Given below is a dictionary having two keys ‘Boys’ and ‘Girls’ and having two lists of heights of five Boys
and Five Girls respectively as values associated with these keys Original dictionary of lists:
{'Boys': [72, 68, 70, 69, 74], 'Girls': [63, 65, 69, 62, 61]}
From the given dictionary of lists create the following list of dictionaries:
[{'Boys': 72, 'Girls': 63}, {'Boys': 68, 'Girls': 65}, {'Boys': 70, 'Girls': 69}, {'Boys': 69, 'Girls': 62},
{‘Boys’:74, ‘Girls’:61] . What are multiple ways to do it? Give at least 3 methods to achieve it. Explain
each method as the comment of your code.

## Method 1: Using a List Comprehension
# Original dictionary of lists
original_dict = {'Boys': [72, 68, 70, 69, 74], 'Girls': [63, 65, 69, 62, 61]}
# Creating the list of dictionaries using list comprehension
result_list = [{'Boys': boys_height, 'Girls': girls_height} for boys_height, girls_height in zip(original_dict['Boys'], original_dict['Girls'])]
# Printing the result
print(result_list)
## Method 2: Using the `zip` function and a For Loop
# Original dictionary of lists
original_dict = {'Boys': [72, 68, 70, 69, 74], 'Girls': [63, 65, 69, 62, 61]}
# Creating an empty list to store dictionaries
result_list = []
# Using zip and a for loop to iterate through the heights
for boys_height, girls_height in zip(original_dict['Boys'], original_dict['Girls']):
    result_list.append({'Boys': boys_height, 'Girls': girls_height})
# Printing the result
print(result_list)
## Method 3: Using the `map` function and Lambda Expression
# Original dictionary of lists
original_dict = {'Boys': [72, 68, 70, 69, 74], 'Girls': [63, 65, 69, 62, 61]}
# Using map and a lambda expression to create the list of dictionaries
result_list = list(map(lambda x, y: {'Boys': x, 'Girls': y}, original_dict['Boys'], original_dict['Girls']))
# Printing the result
print(result_list)



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Q3.Create a dataframe having at least 5 columns and 100 rows to store numeric data generated using a random
function. Replace 25% of the values by null values whose index positions are generated using random
function. Do the following:
a. Identify and count missing values in a dataframe.
b. Drop the column having more than 5 null values.
c. Identify the row label having maximum of the sum of all values in a row and drop that row.
d. Sort the data frame on the basis of the first column.
e. Remove all duplicates from the first column.
f. Find the correlation between first and second column and covariance between second and third column.
g. Detect the outliers and remove the rows having outliers.
h. Discretize second column and create 5 bins

import pandas as pd
import numpy as np

# Step 1: Create a dataframe with random numeric data and introduce null values
np.random.seed(42)
data = np.random.randn(100, 5)
df = pd.DataFrame(data, columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5'])

# Introduce null values randomly
null_indices = np.random.choice(df.size, int(0.25 * df.size), replace=False)
df.values.ravel()[null_indices] = np.nan

# Display the dataframe
print("Original DataFrame:")
print(df.head())

# Step 2: Identify and count missing values
missing_values = df.isnull().sum()
print("\na. Missing Values:")
print(missing_values)

# Step 3: Drop columns with more than 5 null values
df = df.dropna(axis=1, thresh=95)
print("\nb. DataFrame after dropping columns with more than 5 null values:")
print(df.head())

# Step 4: Drop the row label with the maximum sum of values
if not df.empty:
    max_sum_row_label = df.sum(axis=1).idxmax()
    df = df.drop(index=max_sum_row_label)
    print("\nc. DataFrame after dropping the row with the maximum sum of values:")
    print(df.head())

# Step 5: Sort the dataframe based on the first column
if not df.empty:
    df = df.sort_values(by=df.columns[0])
    print("\nd. DataFrame after sorting based on the first column:")
    print(df.head())

# Step 6: Remove duplicates from the first column
if not df.empty:
    df = df.drop_duplicates(subset=df.columns[0])
    print("\ne. DataFrame after removing duplicates from the first column:")
    print(df.head())

~~~~~~~~~~~~~~~~~~~~~~~Q4.Consider two excel files having attendance of a workshop’s participants for two days. Each file has three
fields ‘Name’, ‘Time of joining’, duration (in minutes) where names are unique within a file. Note that
duration may take one of three values (30, 40, 50) only. Import the data into two dataframes and do the
following:
a. Perform merging of the two dataframes to find the names of students who had attended the workshop on
both days.
b. Find names of all students who have attended workshop on either of the days.
c. Merge two data frames row-wise and find the total number of records in the data frame.
d. Merge two data frames and use two columns names and duration as multi-row indexes. Generate
descriptive statistics for this multi-index.

import pandas as pd

# Load data from Excel files
file_path_day1 = "attendance_day1.xlsx"
file_path_day2 = "attendance_day2.xlsx"

df_day1 = pd.read_excel(r"C:\Users\taman\OneDrive\Desktop\DAV\attendance_day1.xlsx")
df_day2 = pd.read_excel(r"C:\Users\taman\OneDrive\Desktop\DAV\attendance_day2.xlsx")

# a. Merge to find names of students who attended on both days
both_days_attendance = pd.merge(df_day1, df_day2, on="name", how="inner")

# b. Names of students who attended on either of the days
either_day_attendance = pd.merge(df_day1, df_day2, on="name", how="outer")

# c. Merge row-wise and find the total number of records
total_records = pd.concat([df_day1, df_day2], ignore_index=True)

# d. Merge and use two columns as multi-row indexes, then generate descriptive statistics
merged_multi_index = pd.merge(df_day1, df_day2, on=["name", "duration"], how="outer").set_index(["name", "duration"])
descriptive_stats_multi_index = merged_multi_index.groupby(["name", "duration"]).describe()

# Display results or perform further analysis as needed
print("Names of students who attended on both days:")
print(both_days_attendance["name"].unique())

print("\nNames of students who attended on either of the days:")
print(either_day_attendance["name"].unique())

print("\nTotal number of records after row-wise merge:", len(total_records))

print("\nDescriptive statistics with multi-row indexes:")
print(descriptive_stats_multi_index)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Q5.Consider a data frame containing data about students i.e. name, gender and passing division:

a. Perform one hot encoding of the last two columns of categorical data using the get_dummies() function.
b. Sort this data frame on the “Birth Month” column (i.e. January to December). (Hint: Convert Month to
Categorical.)

import pandas as pd

# Create the DataFrame
data = {
    'Name': ['Mudit Chauhan', 'Seema Chopra', 'Rani Gupta', 'Aditya Narayan', 'Sanjeev Sahni',
             'Prakash Kumar', 'Ritu Agarwal', 'Akshay Goel', 'Meeta Kulkarni', 'Preeti Ahuja',
             'Sunil Das Gupta', 'Sonali Sapre', 'Rashmi Talwar', 'Ashish Dubey', 'Kiran Sharma',
             'Sameer Bansal'],
    'Birth_Month': ['December', 'January', 'March', 'October', 'February', 'December', 'September',
                    'August', 'July', 'November', 'April', 'January', 'June', 'May', 'February',
                    'October'],
    'Gender': ['M', 'F', 'F', 'M', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'F', 'M'],
    'Pass_Division': ['III', 'II', 'I', 'I', 'II', 'III', 'I', 'I', 'II', 'II', 'III', 'I', 'III',
                      'II', 'II', 'I']
}

df = pd.DataFrame(data)

# a. Perform one-hot encoding of the last two columns
df_encoded = pd.get_dummies(df, columns=['Gender', 'Pass_Division'], prefix=['Gender', 'Division'])

# b. Sort the DataFrame based on the "Birth Month" column
month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September',
               'October', 'November', 'December']
df_encoded['Birth_Month'] = pd.Categorical(df_encoded['Birth_Month'], categories=month_order, ordered=True)
df_encoded_sorted = df_encoded.sort_values('Birth_Month')

# Display the resulting DataFrame
print(df_encoded_sorted)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Q6.Consider the following data frame containing a family name, gender of the family member and her/his
monthly income in each record.

Write a program in Python using Pandas to perform the following:
A. Calculate and display familywise gross monthly income.
B. Display the highest and lowest monthly income for each family name.
C. Calculate and display monthly income of all members earning income less than Rs. 80000.00.
D. Calculate and display the average monthly income of the female members in the Shah family.
E. Calculate and display monthly income of all members with income greater than Rs. 60000.00.
F. Display total number of females along with their average monthly income.
G. Delete rows with Monthly income less than the average income of all members

import pandas as pd
# Create the DataFrame
data = {
    'FamilyName': ['Shah', 'Vats', 'Vats', 'Kumar', 'Vats', 'Kumar', 'Shah', 'Shah', 'Kumar', 'Vats'],
    'Gender': ['Male', 'Male', 'Female', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male'],
    'MonthlyIncome': [44000.00, 65000.00, 43150.00, 66500.00, 255000.00, 103000.00, 55000.00, 112400.00, 81030.00, 71900.00]
}

df = pd.DataFrame(data)
# A. Calculate and display familywise gross monthly income.
familywise_income = df.groupby('FamilyName')['MonthlyIncome'].sum()
print("A. Familywise Gross Monthly Income:")
print(familywise_income)
print()
# B. Display the highest and lowest monthly income for each family name.
highest_income = df.groupby('FamilyName')['MonthlyIncome'].max()
lowest_income = df.groupby('FamilyName')['MonthlyIncome'].min()
print("B. Highest Monthly Income:")
print(highest_income)
print("\nB. Lowest Monthly Income:")
print(lowest_income)
print()
# C. Calculate and display monthly income of all members earning income less than Rs. 80000.00.
income_below_80000 = df[df['MonthlyIncome'] < 80000.00]['MonthlyIncome']
print("C. Monthly Income of Members Earning Less Than Rs. 80000.00:")
print(income_below_80000)
print()


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~``Q7.Using the parsed.csv file, complete the following exercises to practise your pandas skills:
a. Find the 95th percentile of earthquake magnitude in Japan using the magType of 'mb'.
b. Find the percentage of earthquakes in Indonesia that were coupled with tsunamis.
c. Get summary statistics for earthquakes in Nevada.
d. Add a column to the dataframe indicating whether or not the earthquake happened in a country or
US state that is on the Ring of Fire. Use Bolivia, Chile, Ecuador, Peru, Costa Rica, Guatemala,
Mexico (be careful not to select New Mexico), Japan, Philippines, Indonesia, New Zealand,
Antarctica (look for Antarctic), Canada, Fiji, Alaska, Washington, California, Russia, Taiwan,
Tonga, and Kermadec Islands.
e. Calculate the number of earthquakes in the Ring of Fire locations and the number outside them.
f. Find the tsunami count along the Ring of Fire.

import pandas as pd
path=r"C:\Users\hp\Desktop\COLLEGE\SEM 3\DAV\parsed.csv"
df=pd.read_csv(path)
# part a: Find the 95th percentile of earthquake magnitude in Japan using magType 'mb'
Japan = df[(df['magType'] == 'mb') & (df['place'].str.contains('Japan'))]['mag'].quantile(0.95)
print(f"95th Percentile of earthquake magnitude in Japan (magType 'mb'): {Japan}")


# part b: Find the percentage of earthquakes in Indonesia coupled with tsunamis
indonesia_tsunami = (df[(df['place'].str.contains('Indonesia')) & (df['tsunami'] == 1)].shape[0] / df[df['place'].str.contains('Indonesia')].shape[0]) * 100
print(f"Percentage of earthquakes in Indonesia with tsunamis: {indonesia_tsunami:.2f}%")


# part c: Get summary statistics for earthquakes in Nevada
nevada_earthquakes_summary = df[df['place'].str.contains('Nevada')]['mag'].describe()
print("\n Summary statistics for earthquakes in Nevada:")
print(nevada_earthquakes_summary)


# part d: Add a column indicating whether the earthquake happened in a Ring of Fire location
ring_of_fire_locations = ['Bolivia', 'Chile', 'Ecuador', 'Peru', 'Costa Rica', 'Guatemala', 'Mexico',
                          'Japan', 'Philippines','Indonesia', 'New Zealand', 'Antarctica', 'Canada', 
                          'Fiji', 'Alaska', 'Washington', 'California',
                            'Russia', 'Taiwan', 'Tonga', 'Kermadec Islands']


df['RingOfFire'] = df['place'].apply(lambda x: any(location in x for location in ring_of_fire_locations))


# part e: Calculate the number of earthquakes in and outside the Ring of Fire
ring_of_fire = df[df['RingOfFire']].shape[0]
outside_ring_of_fires = df[~df['RingOfFire']].shape[0]
print(f"\n Earthquakes in the Ring of Fire: {ring_of_fire}\n Earthquakes outside the Ring of Fire: {outside_ring_of_fires}")


# Task f: Find the tsunami count along the Ring of Fire
tsunami_count = df[df['RingOfFire']]['tsunami'].sum()
print(f"\n Tsunami count along the Ring of Fire: {tsunami_count}")



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`Q8.Using the CSV files in the earthquakes.csv folder, Write a program in Python using Pandas to perform the
following: :
a. With the earthquakes.csv file, select all the earthquakes in Japan with a magType of mb and a
magnitude of 4.9 or greater.
b. Create bins for each full number of magnitude (for example, the first bin is 0-1, the second is 1-2,
and so on) with a magType of ml and count how many are in each bin.
c. Build a crosstab with the earthquake data between the tsunami column and the magType column.
Rather than showing the frequency count, show the maximum magnitude that was observed for
each combination. Put the magType along the columns.

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
path="/content/drive/MyDrive/dav/earthquakes.csv"
df = pd.read_csv(path)

# part a: applying & to take the intersection of value or where these three conditions match
japan_earthquakes = df[(df['magType'] == 'mb') & (df['mag'] >= 4.9) & (df['place'].str.contains('Japan'))]
print("Earthquakes in Japan with magType 'mb' and magnitude >= 4.9:")
print(japan_earthquakes)

# part b: Create bins for each full number of magnitude (for example, the first bin is 0-1, the second is 1-2,and so on) with a magType of ml and count how many are in each bin
ml_bins = [i for i in range(0, int(df['mag'].max()) + 2)]  # Create bins by using list comprehension
ml_bin_counts = pd.cut(df[df['magType'] == 'ml']['mag'], bins=ml_bins, right=False).value_counts()  #couting number of bins

# displaying counts of bin
print("\nBin counts for magType 'ml':")
print(ml_bin_counts)

# part c: Build a crosstab with max magnitude for each combination of tsunami and magType
cross_tab = pd.crosstab(index=df['tsunami'], columns=df['magType'],values=df['mag'], aggfunc='max', margins=True, margins_name='Max Magnitude')

# displaying crosstab
print("\nCrosstab with max magnitude for tsunami and magType:")
print(cross_tab)




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~``Q9.Using the faang.csv file, group by the ticker and resample to monthly frequency. Make the following
aggregations:
a. Mean of the opening price
b. Maximum of the high price
c. Minimum of the low price
d. Mean of the closing price
e. Sum of the volume traded


from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
path='/content/drive/MyDrive/dav/faang.csv'
df=pd.read_csv(path)
# 'date' column to datetime
df['date'] = pd.to_datetime(df['date'])
# 'date' column as the index
df.set_index('date', inplace=True)
# Groupin by 'ticker' and resample to monthly frequency
monthly_data = df.groupby('ticker').resample('M').agg({ 'open': 'mean','high': 'max','low': 'min','close': 'mean', 'volume': 'sum' }).reset_index()
# M is monthly frequency
print(monthly_data)



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`Q10. Using Titanic dataset, to do the following:
a. Clean the data by dropping the column which has the largest number of missing values.
b. Find total number of passengers with age more than 30
c. Find total fare paid by passengers of second class
d. Compare number of survivors of each passenger class
e. Compute descriptive statistics for age attribute gender wise
f. Draw a scatter plot for passenger fare paid by Female and Male passengers separately
g. Compare density distribution for features age and passenger fare
h. Draw the pie chart for three groups labelled as class 1, class 2, class 3 respectively displayed in different
colours. The occurrence of each group converted into percentage should be displayed in the pie chart.
Appropriately Label the chart.
i. Find % of survived passengers for each class and answer the question “Did class play a role in survival?”


import pandas as pd
import matplotlib.pyplot as plt
# path=r"notebooks/titanic.csv"
# titanic dataset in this code is referred as df
df=pd.read_csv('titanic.csv')
# a. Clean the data by dropping the column with the largest number of missing values
df_cleaned = df.dropna(axis=1, thresh=len(df) * 0.8)
# b. Find total number of passengers with age more than 30
pass_age_over_30 = df_cleaned[df_cleaned['Age'] > 30]
total_pass_over_30 = len(pass_age_over_30)
# c. Find total fare paid by passengers of second class
total_fare_second_class = df_cleaned[df_cleaned['Pclass'] == 2]['Fare'].sum()
# d. Compare the number of survivors of each passenger class
survivors_class = df_cleaned.groupby('Pclass')['Survived'].sum()
# e. Compute descriptive statistics for age attribute gender-wise
descriptive_stats_age = df_cleaned.groupby('Sex')['Age'].describe()
# f. Draw a scatter plot for passenger fare paid by Female and Male passengers separately
plt.figure(figsize=(10, 6))
plt.scatter(df_cleaned[df_cleaned['Sex'] == 'female']['Fare'],
            df_cleaned[df_cleaned['Sex'] == 'female']['Sex'],
            label='Female', alpha=0.5)
plt.scatter(df_cleaned[df_cleaned['Sex'] == 'male']['Fare'],
            df_cleaned[df_cleaned['Sex'] == 'male']['Sex'],
            label='Male', alpha=0.5)
plt.xlabel('Fare')
plt.ylabel('Sex')
plt.title('Scatter Plot of Fare Paid by Gender')
plt.legend()
plt.show()
# g. Compare density distribution for features age and passenger fare
plt.figure(figsize=(12, 6))
plt.hist(df_cleaned['Age'], label='Age', alpha=0.5, bins=30)
plt.hist(df_cleaned['Fare'], label='Fare', alpha=0.5, bins=30)
plt.title('Density distribution for features age and passenger fare')
plt.legend()
plt.show()
# h. Draw the pie chart for three groups labelled as class 1, class 2, class 3 respectively
class_distribution = df_cleaned['Pclass'].value_counts()
labels = ['Class 1', 'Class 2', 'Class 3']
colors = ['green', 'gold', 'red']
plt.pie(class_distribution, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Passenger Classes')
plt.axis('equal')
plt.show()
# i. Find % of survived passengers for each class
survival_perc_class = (df_cleaned.groupby('Pclass')['Survived'].sum() / df_cleaned.groupby('Pclass')['Survived'].count()) * 100
print("Survival Percentage by Class:")
print(survival_perc_class)




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Q11. Using Iris data, plot the following with proper legend and axis labels: (Download IRIS data from:
https://archive.ics.uci.edu/ml/datasets/iris or import it from sklearn datasets.)
a. Load data into Pandas’ data frame. Use the pandas.info() method to look at the info on datatypes in the
dataset.
b. Find the number of missing values in each column (Check number of null values in a column using
df.isnull().sum())
c. Plot bar chart to show the frequency of each class label in the data.
d. Draw a scatter plot for Petal Length vs Sepal Length and fit a regression line
e. Plot density distribution for feature Petal width.
f. Use a pair plot to show pairwise bivariate distribution in the Iris Dataset.
g. Draw heatmap for any two numeric attributes
h. Compute mean, mode, median, standard deviation, confidence interval and standard error for each
numeric feature
i. Compute correlation coefficients between each pair of features and plot heatmap.



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from scipy.stats import t
# a. Load data into Pandas’ data frame. Use the pandas.info() method to look at the info on datatypes in the dataset.
iris = load_iris()
df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])
print("Info about datatypes in the dataset:")
print(df.info())
# b. Find the number of missing values in each column
missing_values = df.isnull().sum()
print("\nNumber of missing values in each column:")
print(missing_values)
# c. Plot bar chart to show the frequency of each class label
class_frequency = df['target'].value_counts()
plt.bar(class_frequency.index, class_frequency.values)
plt.xlabel('Class Label')
plt.ylabel('Frequency')
plt.title('Frequency of Each Class Label')
plt.show()
# d. Draw a scatter plot for Petal Length vs Sepal Length and fit a regression line
plt.scatter(df['sepal length (cm)'], df['petal length (cm)'])
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Petal Length (cm)')
plt.title('Scatter Plot: Petal Length vs Sepal Length')
plt.show()
# e. Plot density distribution for feature Petal width
plt.hist(df['petal width (cm)'], density=True, alpha=0.75)
plt.xlabel('Petal Width (cm)')
plt.ylabel('Density')
plt.title('Density Distribution of Petal Width')
plt.show()
# f. Use a pair plot to show pairwise bivariate distribution in the Iris Dataset.
colors = ['red', 'green', 'blue']
for i in range(3):
    plt.scatter(df[df['target'] == i]['sepal length (cm)'],
                df[df['target'] == i]['sepal width (cm)'],
                label=f'Class {i}', c=colors[i])
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.title('Pairwise Bivariate Distribution in Iris Dataset')
plt.legend()
plt.show()
# g. Draw heatmap for any two numeric attributes
numeric_attributes = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
numeric_attributes_subset = df[numeric_attributes]
correlation_matrix = numeric_attributes_subset.corr()
plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.xticks(range(len(numeric_attributes)), numeric_attributes, rotation=45)
plt.yticks(range(len(numeric_attributes)), numeric_attributes)
plt.title('Heatmap for Numeric Attributes')
plt.show()
# h. Compute mean, mode, median, standard deviation, confidence interval and standard error for each numeric feature

iris = load_iris()
iris_data = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names'] + ['target'])
numeric_attributes = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
numeric_stats = pd.DataFrame({
    'mean': iris_data[numeric_attributes].mean(),
    'mode': iris_data[numeric_attributes].mode().iloc[0],  # Mode may have multiple values
    'median': iris_data[numeric_attributes].median(),
    'std': iris_data[numeric_attributes].std(),
})
standard_error = iris_data[numeric_attributes].sem()
numeric_stats['standard_error'] = standard_error
confidence_level = 0.95
confidence_interval = pd.DataFrame()

for attribute in numeric_attributes:
    mean = numeric_stats.loc[attribute, 'mean']
    std_error = numeric_stats.loc[attribute, 'standard_error']
    df = len(iris_data[attribute]) - 1  # degrees of freedom
    t_value = t.ppf((1 + confidence_level) / 2, df)
    margin_of_error = t_value * std_error
    lower_bound = mean - margin_of_error
    upper_bound = mean + margin_of_error
    confidence_interval[attribute + '_lower'] = [lower_bound]
    confidence_interval[attribute + '_upper'] = [upper_bound]
numeric_stats = pd.concat([numeric_stats, confidence_interval], axis=1)

# Print the results
print("\nDescriptive Statistics and Confidence Intervals for Numeric Features:")
print(numeric_stats)
# i. Compute correlation coefficients between each pair of features and plot heatmap
correlation_coefficients = iris_data.corr()
plt.imshow(correlation_coefficients, cmap='coolwarm', interpolation='none')
plt.colorbar()
plt.xticks(range(len(iris_data.columns)), iris_data.columns, rotation=45)
plt.yticks(range(len(iris_data.columns)), iris_data.columns)
plt.title('Correlation Coefficients Heatmap')
plt.show()



~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~PRACTICAL
~~~~~~~~~~~~~~~1. Write programs in Python using NumPy library to do the following:
a. Create a two dimensional array, ARR1 having random values from 0 to 1. Compute the mean, standard 
deviation, and variance of ARR1 along the second axis.
b. Create a 2-dimensional array of size m x n integer elements, also print the shape, type and data type of 
the array and then reshape it into an n x m array, where n and m are user inputs given at the run time.
c. Test whether the elements of a given 1D array are zero, non-zero and NaN. Record the indices of these 
elements in three separate arrays.
d. Create three random arrays of the same size: Array1, Array2 and Array3. Subtract Array 2 from Array3 
and store in Array4. Create another array Array5 having two times the values in Array1. Find Covariance and Correlation of Array1 with Array4 and Array5 respectively.
e. Create two random arrays of the same size 10: Array1, and Array2. Find the sum of the first half of both 
the arrays and product of the second half of both the arrays.
f. Create an array with random values. Determine the size of the memory occupied by the array.
g. Create a 2-dimensional array of size m x n having integer elements in the range (10,100). Write 
statements to swap any two rows, reverse a specified column and store updated array in another 
variable


import numpy as np
arr1 = np.random.rand(3, 4)
mean_arr1 = np.mean(arr1, axis=1)
std_arr1 = np.std(arr1, axis=1)
var_arr1 = np.var(arr1, axis=1)

print("a. Arr1:")
print(arr1)
print("Mean along axis 1:", mean_arr1)
print("Standard Deviation along axis 1:", std_arr1)
print("Variance along axis 1:", var_arr1)
print()
# b. Create a 2D array, print shape, type, and data type, then reshape based on user inputs
m = int(input("Enter number of rows (m): "))
n = int(input("Enter number of columns (n): "))
arr2 = np.random.randint(0, 10, size=(m, n))
print("b. Original Array:")
print(arr2)
print("Shape:", arr2.shape)
print("Type:", type(arr2))
print("Data Type:", arr2.dtype)

# Check if the new shape is valid
n_new = int(input("Enter new number of columns (n): "))
m_new = int(input("Enter new number of rows (m): "))
if m * n != m_new * n_new:
    print("Invalid new shape. The total number of elements must remain the same.")
else:
    arr2_reshaped = np.reshape(arr2, (m_new, n_new))
    print("Reshaped Array:")
    print(arr2_reshaped)
    print()
# c. Test whether elements of a 1D array are zero, non-zero, and NaN, and record indices
arr3 = np.array([1, 0, 5, 0, np.nan, 8])
zero_indices = np.where(arr3 == 0)[0]
nonzero_indices = np.where(arr3 != 0)[0]
nan_indices = np.where(np.isnan(arr3))[0]

print("c. Array:")
print(arr3)
print("Zero indices:", zero_indices)
print("Non-zero indices:", nonzero_indices)
print("NaN indices:", nan_indices)
print()
# d. Create three random arrays, perform operations, find Covariance and Correlation
array1 = np.random.rand(5)
array2 = np.random.rand(5)
array3 = np.random.rand(5)

array4 = array3 - array2
array5 = 2 * array1

covariance = np.cov(array1, array4)[0, 1]
correlation = np.corrcoef(array1, array5)[0, 1]

print("d. Arrays:")
print("Array1:", array1)
print("Array2:", array2)
print("Array3:", array3)
print("Array4 (Array3 - Array2):", array4)
print("Array5 (2 * Array1):", array5)
print("Covariance between Array1 and Array4:", covariance)
print("Correlation between Array1 and Array5:", correlation)
print()
# e. Create two random arrays, find sum of the first half and product of the second half
array6 = np.random.rand(10)
array7 = np.random.rand(10)

sum_first_half = np.sum(array6[:5])
product_second_half = np.prod(array7[5:])

print("e. Arrays:")
print("Array6:", array6)
print("Array7:", array7)
print("Sum of the first half of Array6:", sum_first_half)
print("Product of the second half of Array7:", product_second_half)
print()
# f. Create an array with random values and determine memory size
array8 = np.random.rand(4, 5)
memory_size = array8.nbytes

print("f. Array:")
print(array8)
print("Memory size of the array (in bytes):", memory_size)
print()

# g. Create a 2D array, swap rows, reverse a specified column, and store in another variable
m_g = 4
n_g = 3
array9 = np.random.randint(10, 100, size=(m_g, n_g))
print("g. Original Array:")
print(array9)
# Swap rows 0 and 1
array9_swapped = array9.copy()
array9_swapped[0, :], array9_swapped[1, :] = array9_swapped[1, :], array9_swapped[0, :].copy()

# Reverse column 2
array9_reversed_column = array9_swapped.copy()
array9_reversed_column[:, 2] = array9_reversed_column[::-1, 2]

print("Array after swapping rows 0 and 1:")
print(array9_swapped)
print("Array after reversing column 2:")
print(array9_reversed_column)


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~. Do the following using PANDAS Series:
a. Create a series with 5 elements. Display the series sorted on index and also sorted on values seperately
b. Create a series with N elements with some duplicate values. Find the minimum and maximum ranks 
assigned to the values using ‘first’ and ‘max’ methods
c. Display the index value of the minimum and maximum element of a Series

import pandas as pd

# a. Create a series with 5 elements, display the series sorted on index and values separately
series_a = pd.Series([4, 2, 7, 1, 9], index=['e', 'b', 'd', 'a', 'c'])

# Sort by index
sorted_by_index = series_a.sort_index()

# Sort by values
sorted_by_values = series_a.sort_values()

print("a. Original Series:")
print(series_a)
print("Sorted by Index:")
print(sorted_by_index)
print("Sorted by Values:")
print(sorted_by_values)
print()

# b. Create a series with N elements with some duplicate values, find minimum and maximum ranks
series_b = pd.Series([4, 2, 7, 1, 9, 2, 7], name='Values')

# Find minimum rank using 'first' method
min_rank_first = series_b.rank(method='first', ascending=True).min()

# Find maximum rank using 'max' method
max_rank_max = series_b.rank(method='max', ascending=True).max()

print("b. Series with Duplicate Values:")
print(series_b)
print("Minimum Rank (using 'first' method):", min_rank_first)
print("Maximum Rank (using 'max' method):", max_rank_max)
print()

# c. Display the index value of the minimum and maximum element of a Series
series_c = pd.Series([4, 2, 7, 1, 9], index=['e', 'b', 'd', 'a', 'c'])

# Index of minimum element
min_index = series_c.idxmin()

# Index of maximum element
max_index = series_c.idxmax()

print("c. Series:")
print(series_c)
print("Index of Minimum Element:", min_index)
print("Index of Maximum Element:", max_index)
    

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~```Consider two excel files having attendance of two workshops, each of duration 5 days. Each file has three 
fields ‘Name’, ‘Date, duration (in minutes) where names may be repetitve within a file. Note that duration may 
take one of three values (30, 40, 50) only. Import the data into two data frames and do the following:
a. Perform merging of the two data frames to find the names of students who had attended both 
workshops.
b. Find names of all students who have attended a single workshop only.
c. Merge two data frames row-wise and find the total number of records in the data frame.
d. Merge two data frames row-wise and use two columns viz. names and dates as multi-row indexes. 
Generate descriptive statistics for this hierarchical data frame.



import pandas as pd

# Load data from Excel files
file_path_day1 = "attendance_day1.xlsx"
file_path_day2 = "attendance_day2.xlsx"

df_day1 = pd.read_excel(r"C:\Users\taman\OneDrive\Desktop\DAV\attendance_day1.xlsx")
df_day2 = pd.read_excel(r"C:\Users\taman\OneDrive\Desktop\DAV\attendance_day2.xlsx")

# a. Merge to find names of students who attended on both days
both_days_attendance = pd.merge(df_day1, df_day2, on="name", how="inner")

# b. Names of students who attended on either of the days
either_day_attendance = pd.merge(df_day1, df_day2, on="name", how="outer")

# c. Merge row-wise and find the total number of records
total_records = pd.concat([df_day1, df_day2], ignore_index=True)

# d. Merge and use two columns as multi-row indexes, then generate descriptive statistics
merged_multi_index = pd.merge(df_day1, df_day2, on=["name", "duration"], how="outer").set_index(["name", "duration"])
descriptive_stats_multi_index = merged_multi_index.groupby(["name", "duration"]).describe()

# Display results or perform further analysis as needed
print("Names of students who attended on both days:")
print(both_days_attendance["name"].unique())

print("\nNames of students who attended on either of the days:")
print(either_day_attendance["name"].unique())

print("\nTotal number of records after row-wise merge:", len(total_records))

print("\nDescriptive statistics with multi-row indexes:")
print(descriptive_stats_multi_index)
